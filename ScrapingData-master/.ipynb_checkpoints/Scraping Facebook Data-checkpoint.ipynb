{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-fca172345f32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import datetime\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "INPUTS:\n",
    "    text: a string\n",
    "OUTPUTS: \n",
    "    a better formatted string\n",
    "\"\"\"\n",
    "def unicode_normalize(text):\n",
    "    return text.translate({ 0x2018:0x27, 0x2019:0x27, 0x201C:0x22, 0x201D:0x22,\n",
    "                            0xa0:0x20 }).encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "INPUTS:\n",
    "    url: a request url\n",
    "OUTPUTS: \n",
    "    the data returned by calling that url\n",
    "\"\"\"\n",
    "def request_data_from_url(url):\n",
    "    req = urllib2.Request(url)\n",
    "    success = False\n",
    "    while success is False:\n",
    "        try: \n",
    "            #open the url\n",
    "            response = urllib2.urlopen(req)\n",
    "            \n",
    "            #200 is the success code for http\n",
    "            if response.getcode() == 200:\n",
    "                success = True\n",
    "        except Exception, e:\n",
    "            #if we didn't get a success, then print the error and wait 5 seconds before trying again\n",
    "            print e\n",
    "            time.sleep(5)\n",
    "\n",
    "            print \"Error for URL %s: %s\" % (url, datetime.datetime.now())\n",
    "            print \"Retrying...\"\n",
    "\n",
    "    #return the contents of the response\n",
    "    return response.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "INPUTS: \n",
    "    page_id: the unique id for the facebook page you are trying to scrape\n",
    "    access_token: authentication proving that you have a valid facebook account\n",
    "OUTPUTS:\n",
    "    a python dictionary of the data on your requested page\n",
    "\"\"\"\n",
    "def get_facebook_page_data(page_id, access_token):\n",
    "\n",
    "    website = \"https://graph.facebook.com/v2.6/\"\n",
    "    \n",
    "    location = \"%s/posts/\" % page_id \n",
    "    \n",
    "    #the .limit(0).summary(true) is used to get a summarized count of all the ... \n",
    "    #...comments and reactions instead of getting each individual one\n",
    "    fields = \"?fields=message,created_time,type,name,id,\" + \\\n",
    "            \"comments.limit(0).summary(true),shares,\" + \\\n",
    "            \"reactions.limit(0).summary(true)\"\n",
    "            \n",
    "    authentication = \"&limit=100&access_token=%s\" % (access_token)\n",
    "    \n",
    "    request_url = website + location + fields + authentication\n",
    "\n",
    "    #converts facebook's response to a python dictionary to easier manipulate later\n",
    "    data = json.loads(request_data_from_url(request_url))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "INPUTS: \n",
    "    post: information about a single post on the facebook page\n",
    "    access_token: authentication proving that you have a valid facebook account\n",
    "OUTPUTS: \n",
    "    a list with the requested fields for this post\n",
    "\"\"\"\n",
    "def process_post(post, access_token):\n",
    "\n",
    "    post_id = post['id']\n",
    "    \n",
    "    post_message = '' if 'message' not in post.keys() else \\\n",
    "            unicode_normalize(post['message'])\n",
    "        \n",
    "    post_type = post['type']\n",
    "\n",
    "    #for datetime info, we need a few extra steps\n",
    "    #first convert the given datetime into the format we want\n",
    "    post_published = datetime.datetime.strptime(\n",
    "            post['created_time'],'%Y-%m-%dT%H:%M:%S+0000')\n",
    "    #then account for the time difference between the returned time and my time zone\n",
    "    post_published = post_published + \\\n",
    "            datetime.timedelta(hours=-2)\n",
    "    #last, convert the datetime into a string in a format convenient for spreadsheets\n",
    "    post_published = post_published.strftime(\n",
    "            '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    num_reactions = 0 if 'reactions' not in post else \\\n",
    "            post['reactions']['summary']['total_count']\n",
    "    num_comments = 0 if 'comments' not in post else \\\n",
    "            post['comments']['summary']['total_count']\n",
    "    num_shares = 0 if 'shares' not in post else post['shares']['count']\n",
    "\n",
    "    #here we call a separate API for information about reactions based on the post's post_id\n",
    "    #but only if this post is afer the day when reactions first appeared on facebook\n",
    "    reactions = get_reactions_for_post(post_id, access_token) if \\\n",
    "            post_published > '2016-02-24 00:00:00' else {}\n",
    "\n",
    "    num_likes = 0 if 'like' not in reactions else \\\n",
    "            reactions['like']['summary']['total_count']\n",
    "\n",
    "    #if this post is from before reactions existed, then simply set the number of likes ...\n",
    "    #...equal to the total number of reactions\n",
    "    num_likes = num_reactions if post_published < '2016-02-24 00:00:00' \\\n",
    "            else num_likes\n",
    "\n",
    "    #function to get total number of reactions from the reactions dictionary above\n",
    "    def get_num_total_reactions(reaction_type, reactions):\n",
    "        if reaction_type not in reactions:\n",
    "            return 0\n",
    "        else:\n",
    "            return reactions[reaction_type]['summary']['total_count']\n",
    "\n",
    "    #get counts of all reactions\n",
    "    num_loves = get_num_total_reactions('love', reactions)\n",
    "    num_wows = get_num_total_reactions('wow', reactions)\n",
    "    num_hahas = get_num_total_reactions('haha', reactions)\n",
    "    num_sads = get_num_total_reactions('sad', reactions)\n",
    "    num_angrys = get_num_total_reactions('angry', reactions)\n",
    "\n",
    "    #return a list of all the fields we asked for\n",
    "    return (post_id, post_message, post_type,\n",
    "            post_published, num_reactions, num_comments, num_shares,\n",
    "            num_likes, num_loves, num_wows, num_hahas, num_sads, num_angrys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "INPUTS:\n",
    "    post_id: a post id corresponding to a particular post\n",
    "    access_token: authentication proving that you have a valid facebook account\n",
    "OUTPUTS:\n",
    "    a python dictionary of information about the reactions associated to this post\n",
    "\"\"\"\n",
    "def get_reactions_for_post(post_id, access_token):\n",
    "\n",
    "    website = \"https://graph.facebook.com/v2.6\"\n",
    "    \n",
    "    location = \"/%s\" % post_id\n",
    "    \n",
    "    #here we ask for the number of reactions of each time associated with this post\n",
    "    reactions = \"/?fields=\" \\\n",
    "            \"reactions.type(LIKE).limit(0).summary(total_count).as(like)\" \\\n",
    "            \",reactions.type(LOVE).limit(0).summary(total_count).as(love)\" \\\n",
    "            \",reactions.type(WOW).limit(0).summary(total_count).as(wow)\" \\\n",
    "            \",reactions.type(HAHA).limit(0).summary(total_count).as(haha)\" \\\n",
    "            \",reactions.type(SAD).limit(0).summary(total_count).as(sad)\" \\\n",
    "            \",reactions.type(ANGRY).limit(0).summary(total_count).as(angry)\"\n",
    "    \n",
    "    authentication = \"&access_token=%s\" % access_token\n",
    "    \n",
    "    request_url = website + location + reactions + authentication\n",
    "\n",
    "    # retrieve data and store in python dictionary\n",
    "    data = json.loads(request_data_from_url(request_url))\n",
    "     \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "INPUTS:\n",
    "    page_id: the unique id for the facebook page you are trying to scrape\n",
    "    access_token: authentication proving that you have a valid facebook account\n",
    "OUTPUTS:\n",
    "    nothing, simply prints how many posts were processed and how long it took\n",
    "\"\"\"\n",
    "def scrape_facebook_page(page_id, access_token):\n",
    "    #open up a csv (comma separated values) file to write data to\n",
    "    with open('%s_facebook_posts.csv' % page_id, 'wb') as file:\n",
    "        #let w represent our file\n",
    "        w = csv.writer(file)\n",
    "        \n",
    "        #write the header row\n",
    "        w.writerow([\"post_id\", \"post_message\", \"post_type\",\n",
    "                    \"post_published\", \"num_reactions\", \n",
    "                    \"num_comments\", \"num_shares\", \"num_likes\", \"num_loves\", \n",
    "                    \"num_wows\", \"num_hahas\", \"num_sads\", \"num_angrys\"])\n",
    "\n",
    "        has_next_page = True\n",
    "        num_processed = 0  \n",
    "        scrape_starttime = datetime.datetime.now()\n",
    "\n",
    "        print \"Scraping %s Facebook Page: %s\\n\" % (page_id, scrape_starttime)\n",
    "\n",
    "        #get first batch of posts\n",
    "        posts = get_facebook_page_data(page_id, access_token)\n",
    "\n",
    "        #while there is another page of posts to process\n",
    "        while has_next_page:\n",
    "            #we just limit to 200 posts for simplicity, if you want all the posts, just remove this\n",
    "            if num_processed == 200:\n",
    "                break\n",
    "                \n",
    "            #for each individual post in our retrieved posts ...\n",
    "            for post in posts['data']:\n",
    "\n",
    "                #...get post info and write to our spreadsheet\n",
    "                w.writerow(process_post(post, access_token))\n",
    "                    \n",
    "                num_processed += 1\n",
    "\n",
    "            #if there is a next page of posts to get, then get next page to process\n",
    "            if 'paging' in posts.keys():\n",
    "                posts = json.loads(request_data_from_url(\n",
    "                                        posts['paging']['next']))\n",
    "            #otherwise, we are done!\n",
    "            else:\n",
    "                has_next_page = False\n",
    "\n",
    "\n",
    "        print \"Completed!\\n%s posts Processed in %s\" % \\\n",
    "                (num_processed, datetime.datetime.now() - scrape_starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please Paste Public Page Name:marvelstudios\n",
      "Please Paste Your Access Token:EAACEdEose0cBAICOyBXaLZCd8ze1xQEGttl4EPV1bOi2KSlStFMcp94Up0c6ZAyedW8qUmqTAJOOZAcZChvx8iNE9XiTOW6A3RgmFZBLOBs5Yw7mG64lisb2QYBtZCe341T3w2JQcOqyxnCFY8WG25OblfH9XZCOB1fBKU0dd7YBbhJfMQogPBZAQgq2UXFmVHgZD\n",
      "Scraping marvelstudios Facebook Page: 2018-05-28 08:06:11.186000\n",
      "\n",
      "Completed!\n",
      "200 posts Processed in 0:01:00.739000\n"
     ]
    }
   ],
   "source": [
    "page_id = raw_input(\"Please Paste Public Page Name:\")\n",
    "\n",
    "access_token = raw_input(\"Please Paste Your Access Token:\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    scrape_facebook_page(page_id, access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
